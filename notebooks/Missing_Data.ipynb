{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T12:52:17.866640Z",
     "start_time": "2019-04-17T12:52:17.310408Z"
    }
   },
   "source": [
    "<h1 align=\"center\">Building a better embedding</h1>\n",
    "\n",
    "## Data for embeddings\n",
    "\n",
    "When representing materials in ML models there is a hierachy of structure; Target property > Material Embedding > Atomic Embeddings. Current Atomics embeddings have the scope to include more physics, the principle area's where existing attempts fall short are not including infomation about the form factor or the expected spins of the most abundant isotopes.\n",
    "\n",
    "To try construct a better embedding we have prepared a dataset of atomic features by collecting infomation from multiple sources; Mathematica, Wikipedia, Textbooks and other websites. We discard elements heavier than Californium because the International Tables for Crystallography do not give form factors for heavier elements. \n",
    "\n",
    "Signifcant overlap exists between the infomation contained in our dataset and previous attempts for featurising materials such as [Magpie](http://oqmd.org/static/analytics/magpie/doc/). Magpie takes a list of feature properties to use and carries out an end-to-end model throwing errors it if encounters missing values. This puts emphasis on selecting a feature set that is fully detemined for the problem. \n",
    "\n",
    "In contrast, we want to construct an embedding which means that we need a dense representation where every column is given for each atomic key. This moves the issue of dealing with missing data from run-time to the problem setup phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:53.338414Z",
     "start_time": "2019-04-26T10:04:52.575217Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:53.378324Z",
     "start_time": "2019-04-26T10:04:53.341745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of missing data is 7.79\n",
      "The percentage of dense data is 56.62\n",
      "The percentage of unnecessarily discarded data is 35.59\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"atom_data.csv\", header=None)\n",
    "df = df.set_index(df.columns[0])\n",
    "\n",
    "percent_missing = 100*(df.size - df.count().sum())/df.size\n",
    "print(\"The percentage of missing data is {:.2f}\".format(percent_missing))\n",
    "\n",
    "df_dense =  df.dropna(axis=1, how='any')\n",
    "percent_dense = 100*df_dense.size/df.size\n",
    "print(\"The percentage of dense data is {:.2f}\".format(percent_dense))\n",
    "\n",
    "print(\"The percentage of unnecessarily discarded data is {:.2f}\".format(100-percent_dense-percent_missing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:11.984228Z",
     "start_time": "2019-04-17T13:04:11.972436Z"
    }
   },
   "source": [
    "In order to use the collected atom data as an embedding for machine learning we need a dense representation. We can relatively straightforwardly produce a dense representation by dropping any columns with missing values but we see that doing this results in discarding 35.59% of our data.\n",
    "\n",
    "A better way to obtain a dense representation would be on a problem by problem basis where we first check which elements are present in the data and drop the rows corresponding to elements that are not present first before dropping the columns where we still have missing values. This is essentially what we would be doing implicitly if using Magpie. Evidently reducing the allowed elemet set reduces the ability of the embedding to generalise. Using this improved scheme we may get away with discarding less data but having to discard any data is non-ideal. \n",
    "\n",
    "The alternative to dropping data is to impute values. When imputing it is important to consider why the data is missing, in general there are three types of missing data:\n",
    "\n",
    "* Missing completely at random (MCAR)\n",
    "* Missing at random (MAR)\n",
    "* Missing not at random (MNAR)\n",
    "\n",
    "In practise we onyl consider data MAR and MNAR as for data to be MCAR means requires that it is unrelated to the problem being investigated. In our problem MAR data is data that could have been collected about an atom but hasn't been and MNAR data is data that is missing because it doesn't exist i.e. the P-Zunger radius for Helium.\n",
    "\n",
    "In the first instance we will do a simple linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:53.595339Z",
     "start_time": "2019-04-26T10:04:53.380415Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imputed = df.interpolate('linear', limit_direction='both')\n",
    "\n",
    "# note it is critical here that we normalise our data before the correlation\n",
    "normalized_df=(df_imputed-df_imputed.mean())/df_imputed.std()\n",
    "\n",
    "embedding = normalized_df.T.to_dict('list')\n",
    "\n",
    "with open('elem_embedding.json', 'w') as f:\n",
    "    json.dump(embedding, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T16:08:01.146849Z",
     "start_time": "2019-04-25T16:07:58.698550Z"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Often, particularly when dealing smaller datasets (i.e. real experimental data), we want to ensure that we reduce the dimensionality of out problem as much as possible as this reduces the extent to which our system will be over-parameterised (under-determined).\n",
    "\n",
    "A straightforward and well established dimensionality reduction approach is Principle Componet Analysis (PCA). PCA converts a set of possibly correlated variables into a set of values of linearly uncorrelated variables that are refered to as the principal components of the system. Think of it by analogy to taking the leading terms when projecting onto a orthogonal basis set of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:53.766367Z",
     "start_time": "2019-04-26T10:04:53.598706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "\n",
    "low = pca.fit_transform(normalized_df.values)\n",
    "\n",
    "df_low = pd.DataFrame(low,index=df.index)\n",
    "\n",
    "embedding = df_low.T.to_dict('list')\n",
    "\n",
    "with open('elem_low_embedding.json', 'w') as f:\n",
    "    json.dump(embedding, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
